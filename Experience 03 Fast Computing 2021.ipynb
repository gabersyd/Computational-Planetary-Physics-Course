{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How fast is your Computing Machine?\n",
    "\n",
    "Our computers do basically two things, one is storing numbers, the other is doing mathematical operations on them. They are so useful because they do these things extraordinarily fast and extraordinarily well, in the sense that they always do them correctly following our instructions. For this reason the main way in which we use them is to give them repetitive instructions for up to billions of times.\n",
    "\n",
    "Since the speed of the CPU has doubled every 1.5 years until a few years ago (Moore’s Law), we may have lost track of how really fast the computer in our hands is. When starting this journey into numerical modeling, it is important to first quickly experiment this and not only realize how remarkably fast it really is, but also to perceive its limits when we reach them.\n",
    "\n",
    "In this chapter, we start using iPython because its interactive setting allows us to feel better about what the computer does. Later we will step into writing and running a program. Let us initially load NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's create a little function that simply counts up to a value *max*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this function simply counts from 0 to max\n",
    "def count(max):\n",
    "    i=0\n",
    "    while i<max:\n",
    "        i=i+1\n",
    "    #print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I try to launch count(1000) or count(100000) on my laptop I simply do not observe any difference. Just the time to call the function, and I get the result. It is unnoticeable that the second call required 100 times more calculation than the first one. Only when I arrive at one million or better ten millions, I finally observe a lag. In fact what I notice is the time that it takes for one processor of my laptop to perform this serial calculation. You can test yours and perceive the power of your computer.\n",
    "\n",
    "We can also be more quantitative about the time required by our little routine. iPython offers a very simple and powerful magic command called %timeit. Let us test it for testing the time required by our loop:\n",
    "\n",
    "The %timeit command is used to calculate the average time of performing certain operations in python. As an example let us use this command to calculate the time that it takes to count from 0 to max in the above function. \n",
    "\n",
    "### Exercise 1\n",
    "\n",
    "Use the command %timeit to calculate how long it takes to the function *Count()* to count up to:\n",
    "\n",
    "a) 100000 \n",
    "\n",
    "b) 1000000 \n",
    "\n",
    "c) 10000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#here put your solution for (a) and create two more cells for (b) and (c)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the computing times that you find. Are they proportional to how much the funcion counts? Why? Can you now estimate how fast is your computer? \n",
    "\n",
    "Let's now create a sligtly more sophisticate function, which adds the first *max* numbers instead of simply counting them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# also this function counts, but increasingly larger numbers\n",
    "def addCount(max):\n",
    "    a=0.0\n",
    "    for i in range(max):\n",
    "        a=a+i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Use the command %timeit to calculate how long it takes to the function *addCount()* to sum numbers up to:\n",
    "\n",
    "a) 100000 \n",
    "\n",
    "b) 1000000 \n",
    "\n",
    "c) 10000000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#here put your solution for (a) and create two more cells for (b) and (c)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, comments on the computing times that you find. Here...\n",
    "\n",
    "It takes almost the same! \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NumPy \n",
    "\n",
    "Let's focus on the Numerical Tools of Python. Among others, NumPy adds to standard Python the following features:   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">A. Multidimensional vectorized arrays<br />\n",
    ">B. Mathematical functions operating on an array or portions of it<br />\n",
    ">C. Linear Algebra, Fourier development, Random Functions<br />\n",
    "      D. Input/Output functions to efficiently create and read memory mapped files<br />\n",
    "      \n",
    "Let's start from the NumPy Types "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NumPy Types\n",
    "\n",
    "Without any further specification, Python automatically uses float type machine precision. We might not know which precision this is, in which case it is important to discover it. A very straightforward way to discover the precision of our machine is by testing for which small ε, a and a + ε become indistinguishable. We can do so by iteratively decreasing ε of a certain ratio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon, ratio = 1.0, 9.0 \n",
    "while (epsilon):\n",
    "    print('Precision:',epsilon) \n",
    "    epsilon /= ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last line indicates that the precision that standard Python assumes on my machine is $10^{−324}$. This is indeed associated to a 64-bit float number. One can verify this by setting a smaller precision in the first line of this sequence. \n",
    "\n",
    "### Exercise 3\n",
    "\n",
    "Test with the above routine the precision of the following types:\n",
    "\n",
    "a) np.float16()\n",
    "\n",
    "b) np.float32()\n",
    "\n",
    "Google now all the other types of Float available on NumPy and check which ones work on this machine (why some may not?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to float, there exist integers with several precisions: np.int32, np.int64, np.int128. The type of every NumPy variable can be retrieved the dtype, e.g., eps.dtype.\n",
    "\n",
    "It is very important to keep in mind the importance of precision. All the mathematical routines implemented in Python, as well as in any other language, follow the standards determined by IEEE which imply that they are all corrected to the last digit. Therefore, when we will model a physical system we can be sure that errors will not propagate quickly to lower digits with every algorithm that does not explicitly remove this information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization \n",
    "Vectorization is a technique that allows the programmer to replace the loops in vector/matrix operations with a unique call on an entire vector, or matrix. The vectorization makes the program look simple and also facilites the parallization. \n",
    "\n",
    "Let's consider for example matrix multiplications. Here we will replance loops with vectorized calls. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: matrix multiplication using loop (non-vectorized algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a=np.array([[1,2],[3,4]])\n",
    "b=np.array([[5,6],[7,8]])\n",
    "c=np.zeros((2,2),int)\n",
    "\n",
    "def myMultiplication(a,b,c):\n",
    "    size=np.shape(a)[0]\n",
    "    for i in np.arange(size):\n",
    "        for j in np.arange(size):\n",
    "            sum=0;\n",
    "            for k in np.arange(size):\n",
    "                sum+=a[i, k] * b[k ,j];\n",
    "            c[i,j]=sum\n",
    "    return c\n",
    "\n",
    "%timeit myMultiplication(a,b,c)\n",
    "c= myMultiplication(a,b,c)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: Matrix multiplication using vectorized algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.array([[1,2],[3,4]])\n",
    "b=np.array([[5,6],[7,8]])\n",
    "c=np.matmul(a,b)\n",
    "print(c)\n",
    "\n",
    "size=np.shape(a)[0]\n",
    "print(size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "\n",
    "Create two larger arrays of measure 10 x 10 (for example using arange and reshape, but there are other ways). And multiply them with the two techniques above. If you put them in a function, you can also %timeit these operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ndarrays\n",
    "\n",
    "Let us start looking at the vectorized arrays. The core of NumPy is the object called ndarray, a storage of large quantities of data that allows to operate fast and flexible operations on it. Let’s look at some basic examples using Jupyter in interactive mode and let's create a two-dimensional array of two rows and four columns:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstArr = np.array([[4,6,3,4],[2,3,6,0]], dtype=np.int8)\n",
    "firstArr.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstArr.ndim # number of dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstArr.shape # array shape as (rows,columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstArr.dtype # Out[10]: dtype(’int64’)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we find is that NumPy, if given a certain set of data, automatically decides a type (generally integer or float) and defines an array of the right shape where to store them. Given a NumPy array, it is always possible to obtain its shape and data type (dtype). The main difference between ndarrays and Python data types is that the elements of ndarrays are all of a predefined and homogeneous type, which is an essential property to reach great speed for its calculations.\n",
    "\n",
    "In general, one can set the data type at the moment of creation of the array, which is a particularly useful function when one has to handle with very large models and datasets and needs to be in control of the size of the occupied memory. Data in NumPy are either int: integer, uint: unsigned integer (from 0), and float, a real number. int and uint can have sizes of 8, 16, 32, and 64 bits, while float of 16, 32, and 64 bits. If not otherwise set, automatically NumPy will set data type size to 64 bits.\n",
    "\n",
    "Most commonly used generators of ndarrays are arange, zeros and ones. Let us look at some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secondArr = np.arange((20),dtype=np.int32)\n",
    "secondArr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secondArr.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In [14]: secondArr = np.zeros(20)\n",
    "In [15]: secondArr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In [16]: secondArr.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In [17]: secondArr = np.ones(20)\n",
    "In [18]: secondArr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In [19]: secondArr.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*arange()* is the analogue of *range()* in Python, but it creates a NumPy array of 64 bits integers. *zeros()* and *ones()* instead create arrays of 64 bits float. *ones()* can also be created from *zeros()* using the broadcasting properties of NumPy arrays, just by the instruction *arr=np.zeros(20)+1.0*. If we wish to create arrays with different types, for example due to operational reasons or of memory size, dtypes can be explicitly specified, \n",
    "e.g.,*arr = np.arange(20, dtype=’float32’)* or *arr = np.ones(20, dtype=’int8’)*.\n",
    "\n",
    "Let us now check how using NumPy allows to speed the calculations done earlier. The first temptation might be simply to replace *range()* with *np.arange()*. Will it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "\n",
    "Rewrite the function *addCount()*, replacing *range* with *np.arange* and %timeit this new function. Give it a different name, to make the comparison possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# here you can put your solution\n",
    "def addCount1(max):\n",
    "    a=0.0\n",
    "    for i in range(max):\n",
    "        a=a+i\n",
    "\n",
    "#def addCount2(max):\n",
    "#...\n",
    "#fill here\n",
    "#...\n",
    "\n",
    "#%timeit addCount1(1000000)\n",
    "#%timeit addCount2(1000000)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now time the same quantities of Exercise 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# here run the %timeit commands\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you find? No acceleration?? Interesting!\n",
    "\n",
    "What happened here is that although the arrays were ndarrays, the operation was done as for a standard Python list. The trick to speed calculations on ndarrays is to use the broadcast vectorized version of each operation. Let us look at how to add two ndarrays of one million of integers either with the standard Python loop and exploiting the NumPy broadcasting capabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def addArray(a,b):\n",
    "    c=np.zeros(a.size)\n",
    "    for i in np.arange(a.size):\n",
    "        c[i]=a[i]+b[i]\n",
    "    return(c)\n",
    "\n",
    "a=np.arange(1000000)\n",
    "b=np.arange(1000000)\n",
    "\n",
    "%timeit addArray(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit c=addArray(a,b) #standard python \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6\n",
    "\n",
    "use the broadcasting command *a+b* to sum the two arrays, as done by the routine above. How fast is it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# here you can make your test.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What did you find? A huge gain this time?! Two orders of magnitude!? Uau!\n",
    "\n",
    "To use the broadcasting feature of NumPy makes Python’s speed comparable to compiled codes such as C, but with the obvious gain in terms for code development, testing, and readability. Broadcasting is clearly not limited to the addition operation, but it works as well with all the other arithmetic operations such as c=a*b; c=a/b; c=a-b,as well as scalar-array operations like \n",
    "```python \n",
    "c=1/a; \n",
    "c=a**0.5\n",
    "```\n",
    "Besides operations on two arrays, there are numerous unary (and binary universal functions that can be used with NumPy arrays. Among unary functions, much used ones are \n",
    "```python \n",
    "np.abs()\n",
    "np.sqrt()\n",
    "np.exp()\n",
    "np.log()\n",
    "np.sign()\n",
    "```\n",
    "and all the trigonometric functions. Binary functions are \n",
    "```python \n",
    "np.add()\n",
    "np.multiply()\n",
    "np.power()\n",
    "np.maximum()\n",
    "np.mod().\n",
    "```\n",
    "\n",
    "Let us go back now to the initial problem. We wanted to sum all the elements of a large array (100 millions numbers). This is neither an unary or binary operation, because it is a function that projects an array into one number, the sum. The most common among these operations are already efficiently implemented in NumPy. For example, the sum is immediately obtained by *np.arange(max).sum()*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7\n",
    "\n",
    "use the *sum()* call to sum the first 10000000 numbers. How faster is this operation compared to the routine developed earlier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# here you can make your test\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These operations have been in fact written in Cython, which is a C compiled operation. We will see later in this chapter how such operations, when not already implemented in NumPy, can (not easily!) be created using Cython."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing and Slicing\n",
    "\n",
    "Most languages today are compiled. One of the great features of compilers is that when we have to perform a set of repetitive operations on a large set of data, the compiler will take care to optimize these iterations in an exceptional way, even if we wrote a very confusing code. This is not the case with a language like Python.\n",
    "\n",
    "If we want our Python code to run fast, we have to organize the sequence of operation in a smart way. In particular a very common case like calling an **if** command inside a for loop can make Python very slow. We have seen above how looping makes Python slow. One of the secrets that make Python fast is to use smart indexing. To use indexing, in practice it means to vectorize the command **if** and then to use these indexes to selectively operate on arrays. This might seem a terrible setback for Python programmers, however the great advantage of forcing the developer to do this operation is that the program will be written in a completely vectorized way from the start, therefore ready to be parallelized, which is in fact a huge advantage.\n",
    "\n",
    "Indexing and slicing in NumPy is a long topic, whose full coverage goes beyond the scope of this series of Notebooks. To gain a full understanding of the possibilities offered by NumPy, I recommend to follow one of the online free tutorials (e.g., https://docs.scipy.org/doc/numpy-dev/user/basics.indexing.html). I will cover here some of the main features, and explain in depth important details on the memory management associated to ndarrays.\n",
    "\n",
    "Let us now dive into the main features that interest us. Given an array, e.g., **arr**, we can access the element n with square brackets **arr[n]** and we can slice the array extracting the elements between n and m with the command **arr[n:m]**, which will return m − n elements (comprising **arr[n]** and excluding **arr[m]**. It is very important to understand that even if we associate a name to the slice of arr, this is only a view, not a new array implying that the m − n elements are not copied in a new allocated chunk of memory. This means that by changing the sliced array you will change the initial data. Let us look at an example to clearly understand how this works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secondArr=np.arange(20)\n",
    "secondArr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sliceArr = secondArr[4:10]\n",
    "print(sliceArr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sliceArr[3]=100000\n",
    "sliceArr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secondArr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Who is familiar with other languages might be surprised by this behavior, and believe that when defining sliceArr Python should have copied the subset of second **Arr** data into a new array. The point is that NumPy has been designed to deal with very large datasets or numerical models, therefore it uses a policy of minimization of memory usage. For this reason, if one does not force NumPy to create a copy of the sliced data, it will simply generate a *view* of the already existing array. NumPy can be forced to create a separate new array by adding **.copy()** to the slice, e.g., in our case **sliceArr = secondArr[4:10].copy()**.\n",
    "\n",
    "Slicing is very flexible and allows omitting, for example, the first index (e.g., **arr[:10]**) or the last index (e.g., **arr[10:]**) of a slice, which implies that the slice reaches the end of the array. It is also possible to slice the array every k elements by indicating a third parameter in the slice (e.g., **arr[4:12:3]**), and also omitting the other parameters (e.g., **arr[::3]**). Negative indexes are also admitted, which means that counting starts from the last element, backward, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secondArr[::3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N-Dimensional Indexing\n",
    "Indexing and slicing in more dimensions are just a recursive repetition of one-dimensional operations. In 2D, for example, arrays can be indexed either as **arr2d[n][m]** or, with the same effect, as **arr2d[n,m]**. The first index refers to the inner array, the second index to the outer array. For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr2d = np.arange(20)\n",
    "sliceArr = arr2d[4:10]\n",
    "arr2dnew=arr2d.reshape((5,4))\n",
    "arr2dnew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr2dnew[4,3]=10000\n",
    "print(arr2dnew)\n",
    "arr2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sliceArr[3]=100\n",
    "arr2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr2dnew[:,0]=100.\n",
    "print(arr2dnew)\n",
    "arr2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is very important to remember that the reshape command does not create a copy of the data, as well as slicing. **The data remain stored in the memory as a one-dimensional array, regardless of the dimensions and shape of the array**. The way in which this is done and how NumPy broadcasting operations remain extremely efficient is explained later, where strides are illustrated. \n",
    "\n",
    "Data analysis with Python is a huge topic. A great book that analyzes in greater detail how Python can deal with data is McKinney,W.(2012) Python for data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8\n",
    "\n",
    "Create a (a) 1d, (b) 2d, and (c) 3d dimensional array, and, slice it, change some value of the slice, and verify that that the value of the initial array changed, too. \n",
    "\n",
    "Now, let's create the slice with the command *myArray.copy()* and check what happens when you change the value of an elements of the slice. Why is that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# here you can put your solutions\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boolean Indexing\n",
    "\n",
    "Sometimes you have to operate with *if* commands on a large dataset, but you don't want to create a loop which would immensely slow down the data processing. A practical tool to do that efficiently is to use boolean indexing.\n",
    "\n",
    "Boolean indexing is a fast and efficient way to select, access and operate on subsets of a NumPy arrays. Let us for example create a random 5 × 5 matrix (array) and select only the positive elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr2d = np.random.randn(5,5)\n",
    "print(arr2d)\n",
    "print(arr2d.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr2d>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr2d[arr2d>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we did was to create a 5×5 array whose values were True when the elements were positive, and False in the opposite case. It was then possible to select only those elements. It is important to notice that the extracted elements have lost their 5 × 5 structure. This again results from the fact that Python stores every array, regardless of its shape, as a 1D array.\n",
    "\n",
    "This technique is normally used to operate on a certain subset of an array. For example if we desire to set to 0 all the negative elements of the above array, we can do it with one instruction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr2d[arr2d<0]=0.\n",
    "arr2d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 9\n",
    "\n",
    "Create a 9 x 9 array of random integers between 0 and 9, and set to -1 all the numbers that are less than 5 with one command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# here you can insert your solution\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides compactness and elegance, this technique guarantees an enormous gain in speed. To quantify this advantage, we can compare the time necessary for a standard Python for loop with the Boolean indexing tool described above for a large 2000 × 2000 random array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "arr2d=np.random.randn(2000,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setNegativeValuesToZero(n,m,a): \n",
    "    for i in np.arange(n):\n",
    "        for j in np.arange(m):\n",
    "            if a[i,j]<0:\n",
    "                a[i,j]=0\n",
    "%timeit -n1 -r1 setNegativeValuesToZero(2000,2000,arr2d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10\n",
    "\n",
    "Do the operation of done by the function *setNegativeValuesToZero(n,m,a)* with boolean arrays. How faster than that is it?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# here you can insert your very short solution \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we observe a gain of 2 orders of magnitude by using the NumPy indexing compared to standard Python loops. The message is now loud and clear: never use Python loops for large datasets and large numerical models, but always employ Python indexing/slicing features to achieve compiled code performance for speed and memory management. When not possible, rely on Cython, that we will study soon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transposing and Axis Rotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already encountered the function reshape, which allows to project a 1D array as a generic n-dimensional array. One operation that cannot be achieved by reshaping is however transposing, which corresponds to swapping axis. This is a very important function when operating on 2D and 3D numerical modeling, but it is also simply essential in order to calculate the inner product. For example, to calculate Xtranspose * X , one writes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=10\n",
    "arr2d = np.random.randn(n)\n",
    "arr2d\n",
    "arr2d=arr2d.reshape(5,int(n/5))\n",
    "arr2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr2d.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newArr=np.dot(arr2d.T,arr2d)\n",
    "print(newArr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transposing, however, is not limited to swapping x and y axis. One can rotate axis as well. For example in 3D:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr3d = np.arange(2*3*4).reshape(2,3,4)\n",
    "arr3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr3d2 = arr3d.transpose((2,0,1)).copy()\n",
    "print(arr3d.shape)\n",
    "print(arr3d2.shape)\n",
    "arr3d[1,0,0]=1000\n",
    "arr3d2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transposing, as well as reshaping, is just a specific view of the entire array, therefore **transposing will not create a new set of data**, and when modifying the transposed of an array, one modifies the original array as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 11\n",
    "\n",
    "Calculate the euclidean norm of a 1D array of random numbers  by using transpose to multiply two vectors, and then check that the result is correct using the function *numpy.linalg.norm*. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# here you can insert your solution \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strides\n",
    "\n",
    "The technical details given here help to understand how NumPy deals with very large arrays minimizing memory occupation and maximizing access speed.\n",
    "\n",
    "It is important to emphasize that although an array can have many dimensions, the memory in our computer is structured in a purely sequential way, therefore ultimately the real structure of the array is one-dimensional, and every n-dimensional array will only be a specific view on that 1D array.\n",
    "\n",
    "NumPy is extraordinary powerful in managing n-dimensional arrays, different views, complex subsets, for example\n",
    "\n",
    "```python\n",
    "arr[arr*arr<1]. ```\n",
    "\n",
    "This power is based on the ability to directly address chunks of data by striding across the\n",
    "memory and zooming into small blocks of memory. For example, let us take a random 100 × 100 × 100 array of np.float64 data, equivalent to a 1D array of one million elements, occupying 8 bytes each. A block within the array can be extracted in microseconds, almost regardless to the size of the block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr3d=np.random.randn(100,100,100)\n",
    "print(arr3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit -n1 -r1 newArray=arr3d[45:55,45:55,45:55].copy() #small block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit -n1 -r1 newArray2=arr3d[15:85,15:85,15:85].copy() #large block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we operate on the subset of the array however the operational time increases with the size of the block, although not in a proportional way. For example, to set the values of the subset to zero we need about 20 times more time for a subset that is over 300 times greater:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit -n1 -r1 arr3d[45:55,45:55,45:55]=0 #small block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit -n1 -r1 arr3d[15:85,15:85,15:85]=0 #large block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 12\n",
    "\n",
    "Measure the time necessary to set to zero a block of size 10x10x10, 20x20x20, 30x30x30 etc for the big random array arr3d 100x100x100 illustrated above in this section. Plot the time vs size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# here you can insert your solution\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy accesses n-dimensional arrays and its slices as fast as one-dimensional arrays. To understand how this is achieved, let us consider, for example an array a of 32000 integers and then create a reshaped three-dimensional 20 × 40 × 40 version assigned to b. In our case a is composed by 64-bit (8 bytes) integers, which means that one needs to proceed 8 bytes forward to access the next element along the first axis. The size of every element can be also in general assessed with a.itemsize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a=np.arange(32000)\n",
    "b=a.reshape(20,40,40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a.itemsize,b.itemsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will be stored in memory in a buffer that contains 32000 ascending integers from 0 to 31999. As we have seen before a and b are stored in the same memory block. The way in which NumPy differentiates how to operate on them is by characterizing them by their different strides. Strides are tuples of bytes to step in each dimension when traversing an array. In practice they are the offset in bytes between an element and the neighboring one in every direction. Strides are shown explicitly using the instruction arr.strides:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a.shape,a.strides)\n",
    "print(b.shape,b.strides)\n",
    "print(8*40,8*40*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.strides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last number, 8, of the strides refers to the size of each element in the array, i.e., it is always the itemsize. The other numbers refer to the number of bytes forward necessary to access the next element along the other axes. The total number of bytes is called offset and is calculated as offset = sum(indexes * a.strides). For example, for the initial array a[n] every element is accessed at the position \n",
    "```python\n",
    "offset = n*a.strides[0]\n",
    "```\n",
    ", while b[i,j,k] is accessed calculating its associated memory offset as \n",
    "```python\n",
    "offset = i*b.strides[0]+j*b.strides[1]+k*b.strides[2]\n",
    "```\n",
    "in bytes. To obtain the element location one has to divide by b.itemsize. For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b[2,3,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(2*b.strides[0]+3*b.strides[1]+4*b.strides[2])/b.itemsize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 13\n",
    "\n",
    "Create a four dimensional array from of size 3x7x4x5 and verify that the element (2,5,1,4) is the same both using strides and using the python indeces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# here you can insert your solution\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Vector Product\n",
    "\n",
    "The inner product, np.dot() in NumPy, is the most common operation between arrays. It is equivalent to the inner product between 1-D arrays or to matrix multiplication between 2D arrays. It is defined on n-dimensional arrays as the sum product over the last axis of the first array and the second-to-last of the second array. For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(np.random.rand(1000),b=np.random.rand(1000))/1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "will give you a number close to 0.25, since np.random.rand() is a uniformly extracted random number between 0 and 1. Similarly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(np.random.rand(100,100),b=np.random.rand(100,100))/100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "will result in a 100 × 100 matrix of numbers normally distributed around 0.25. \n",
    "\n",
    "Outer products are called by np.outer() and are an easy way to create an x, y regular mesh:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.arange(0,11,1)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=np.ones(11)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.outer(a,b)\n",
    "#print(x)\n",
    "x2=np.outer(b,a)\n",
    "#print(x2)\n",
    "(x-x2)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=np.outer(b,a)\n",
    "plt.plot(x,y,'o');plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy allows composing large arrays combining blocks of one array scaled by another. This operation is called Kronecker product, and we will intensively use it to build large operators for two-dimensional continuum mechanics. For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mix=np.kron(np.eye(3), np.ones((2,2)))\n",
    "print(mix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 14\n",
    "\n",
    "Using *np.kron()*, *np.ones()*, *np.zeros()*, etc Create a matrix made of 6x6 blocks, each of 3x3 matrix of 0,1,2. You choose the shape of the 3x3 matrix, the coolest you can come up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# here you can insert your solution\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Algebra\n",
    "\n",
    "All the linear algebra tools that we will need are already efficiently implemented through the standard optimized ATLAS LAPACK and BLAS libraries. Ultimately all linear algebra routines expect to operate on a 2-dimensional array. There is also a matrix type in NumPy but its use is discouraged since it is possible to obtain the same result by using arrays only.\n",
    "\n",
    "Linear algebra has many subprograms that run routines for decompositions such as Cholesky, QR and Singular Value. They can be very important for many problems, however they do not apply for the problems that we will address in this volume. The interested reader can refer to the regularly updated SciPy manual at http://docs.scipy.org/doc/numpy/reference/routines.linalg.html. \n",
    "\n",
    "Linear algebra (LA) routines are accessed through the module numpy.linalg:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy.linalg as linalg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An often employed tool is linalg.norm() both for calculating the size of a 1D (vector) and 2D (matrix) arrays. Norm, that is just the square root of the sum of the square of all the elements of a n-dimensional array, could be also calculated by the definition, but the numpy.linalg implementation can be one order of magnitude as more efficient. Let us for example benchmark the calculation of a 100 × 100 array norm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.arange(-1000,1001,1)\n",
    "b=np.arange(2001)\n",
    "x=np.outer(a,b); \n",
    "normVectorized=np.sqrt(np.sum(np.sum(x**2)))\n",
    "%timeit -n1 -r1 normVectorized=np.sqrt(np.sum(np.sum(x**2)))\n",
    "print(normVectorized)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(linalg.norm(x))\n",
    "%timeit -n 1 -r1 linalg.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determinants are often employed in linear algebra because they are related to the invertibility of a matrix, and on whether the associated linear system of equations is solvable (a smaller determinant can indicate that some eigenvalues are close to zero, which makes inversion harder). However, to calculate the determinant of extremely large 2D arrays as we will often do in this volume is computationally so demanding that we will do it only for small problems. Let us calculate the determinant of few random matrices, 10 × 10 and 100 × 100, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dets=np.zeros(10)\n",
    "for i in np.arange(10): dets[i]=linalg.det(np.random.rand(10,10))\n",
    "dets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.arange(10): dets[i]=linalg.det(np.random.rand(100,100))\n",
    "dets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
